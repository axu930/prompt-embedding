{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"]=\"1\"\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from models import llama3_aoc\n",
    "import configs\n",
    "\n",
    "\n",
    "# -----\n",
    "\n",
    "device = \"mps:0\"\n",
    "modelpath = \"meta-llama/Llama-3.2-1B-Instruct\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in hf model \n",
    "hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "    modelpath,    \n",
    ")\n",
    "hf_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_tokens.weight\n",
      "layers.0.mlp.gate_proj.weight\n",
      "layers.0.mlp.up_proj.weight\n",
      "layers.0.mlp.down_proj.weight\n",
      "layers.0.self_attn.q_proj.weight\n",
      "layers.0.self_attn.k_proj.weight\n",
      "layers.0.self_attn.v_proj.weight\n",
      "layers.0.self_attn.o_proj.weight\n",
      "layers.0.input_layernorm.weight\n",
      "layers.0.post_attention_layernorm.weight\n",
      "layers.1.mlp.gate_proj.weight\n",
      "layers.1.mlp.up_proj.weight\n",
      "layers.1.mlp.down_proj.weight\n",
      "layers.1.self_attn.q_proj.weight\n",
      "layers.1.self_attn.k_proj.weight\n",
      "layers.1.self_attn.v_proj.weight\n",
      "layers.1.self_attn.o_proj.weight\n",
      "layers.1.input_layernorm.weight\n",
      "layers.1.post_attention_layernorm.weight\n",
      "layers.2.mlp.gate_proj.weight\n",
      "layers.2.mlp.up_proj.weight\n",
      "layers.2.mlp.down_proj.weight\n",
      "layers.2.self_attn.q_proj.weight\n",
      "layers.2.self_attn.k_proj.weight\n",
      "layers.2.self_attn.v_proj.weight\n",
      "layers.2.self_attn.o_proj.weight\n",
      "layers.2.input_layernorm.weight\n",
      "layers.2.post_attention_layernorm.weight\n",
      "layers.3.mlp.gate_proj.weight\n",
      "layers.3.mlp.up_proj.weight\n",
      "layers.3.mlp.down_proj.weight\n",
      "layers.3.self_attn.q_proj.weight\n",
      "layers.3.self_attn.k_proj.weight\n",
      "layers.3.self_attn.v_proj.weight\n",
      "layers.3.self_attn.o_proj.weight\n",
      "layers.3.input_layernorm.weight\n",
      "layers.3.post_attention_layernorm.weight\n",
      "layers.4.mlp.gate_proj.weight\n",
      "layers.4.mlp.up_proj.weight\n",
      "layers.4.mlp.down_proj.weight\n",
      "layers.4.self_attn.q_proj.weight\n",
      "layers.4.self_attn.k_proj.weight\n",
      "layers.4.self_attn.v_proj.weight\n",
      "layers.4.self_attn.o_proj.weight\n",
      "layers.4.input_layernorm.weight\n",
      "layers.4.post_attention_layernorm.weight\n",
      "layers.5.mlp.gate_proj.weight\n",
      "layers.5.mlp.up_proj.weight\n",
      "layers.5.mlp.down_proj.weight\n",
      "layers.5.self_attn.q_proj.weight\n",
      "layers.5.self_attn.k_proj.weight\n",
      "layers.5.self_attn.v_proj.weight\n",
      "layers.5.self_attn.o_proj.weight\n",
      "layers.5.input_layernorm.weight\n",
      "layers.5.post_attention_layernorm.weight\n",
      "layers.6.mlp.gate_proj.weight\n",
      "layers.6.mlp.up_proj.weight\n",
      "layers.6.mlp.down_proj.weight\n",
      "layers.6.self_attn.q_proj.weight\n",
      "layers.6.self_attn.k_proj.weight\n",
      "layers.6.self_attn.v_proj.weight\n",
      "layers.6.self_attn.o_proj.weight\n",
      "layers.6.input_layernorm.weight\n",
      "layers.6.post_attention_layernorm.weight\n",
      "layers.7.mlp.gate_proj.weight\n",
      "layers.7.mlp.up_proj.weight\n",
      "layers.7.mlp.down_proj.weight\n",
      "layers.7.self_attn.q_proj.weight\n",
      "layers.7.self_attn.k_proj.weight\n",
      "layers.7.self_attn.v_proj.weight\n",
      "layers.7.self_attn.o_proj.weight\n",
      "layers.7.input_layernorm.weight\n",
      "layers.7.post_attention_layernorm.weight\n",
      "layers.8.mlp.gate_proj.weight\n",
      "layers.8.mlp.up_proj.weight\n",
      "layers.8.mlp.down_proj.weight\n",
      "layers.8.self_attn.q_proj.weight\n",
      "layers.8.self_attn.k_proj.weight\n",
      "layers.8.self_attn.v_proj.weight\n",
      "layers.8.self_attn.o_proj.weight\n",
      "layers.8.input_layernorm.weight\n",
      "layers.8.post_attention_layernorm.weight\n",
      "layers.9.mlp.gate_proj.weight\n",
      "layers.9.mlp.up_proj.weight\n",
      "layers.9.mlp.down_proj.weight\n",
      "layers.9.self_attn.q_proj.weight\n",
      "layers.9.self_attn.k_proj.weight\n",
      "layers.9.self_attn.v_proj.weight\n",
      "layers.9.self_attn.o_proj.weight\n",
      "layers.9.input_layernorm.weight\n",
      "layers.9.post_attention_layernorm.weight\n",
      "layers.10.mlp.gate_proj.weight\n",
      "layers.10.mlp.up_proj.weight\n",
      "layers.10.mlp.down_proj.weight\n",
      "layers.10.self_attn.q_proj.weight\n",
      "layers.10.self_attn.k_proj.weight\n",
      "layers.10.self_attn.v_proj.weight\n",
      "layers.10.self_attn.o_proj.weight\n",
      "layers.10.input_layernorm.weight\n",
      "layers.10.post_attention_layernorm.weight\n",
      "layers.11.mlp.gate_proj.weight\n",
      "layers.11.mlp.up_proj.weight\n",
      "layers.11.mlp.down_proj.weight\n",
      "layers.11.self_attn.q_proj.weight\n",
      "layers.11.self_attn.k_proj.weight\n",
      "layers.11.self_attn.v_proj.weight\n",
      "layers.11.self_attn.o_proj.weight\n",
      "layers.11.input_layernorm.weight\n",
      "layers.11.post_attention_layernorm.weight\n",
      "layers.12.mlp.gate_proj.weight\n",
      "layers.12.mlp.up_proj.weight\n",
      "layers.12.mlp.down_proj.weight\n",
      "layers.12.self_attn.q_proj.weight\n",
      "layers.12.self_attn.k_proj.weight\n",
      "layers.12.self_attn.v_proj.weight\n",
      "layers.12.self_attn.o_proj.weight\n",
      "layers.12.input_layernorm.weight\n",
      "layers.12.post_attention_layernorm.weight\n",
      "layers.13.mlp.gate_proj.weight\n",
      "layers.13.mlp.up_proj.weight\n",
      "layers.13.mlp.down_proj.weight\n",
      "layers.13.self_attn.q_proj.weight\n",
      "layers.13.self_attn.k_proj.weight\n",
      "layers.13.self_attn.v_proj.weight\n",
      "layers.13.self_attn.o_proj.weight\n",
      "layers.13.input_layernorm.weight\n",
      "layers.13.post_attention_layernorm.weight\n",
      "layers.14.mlp.gate_proj.weight\n",
      "layers.14.mlp.up_proj.weight\n",
      "layers.14.mlp.down_proj.weight\n",
      "layers.14.self_attn.q_proj.weight\n",
      "layers.14.self_attn.k_proj.weight\n",
      "layers.14.self_attn.v_proj.weight\n",
      "layers.14.self_attn.o_proj.weight\n",
      "layers.14.input_layernorm.weight\n",
      "layers.14.post_attention_layernorm.weight\n",
      "layers.15.mlp.gate_proj.weight\n",
      "layers.15.mlp.up_proj.weight\n",
      "layers.15.mlp.down_proj.weight\n",
      "layers.15.self_attn.q_proj.weight\n",
      "layers.15.self_attn.k_proj.weight\n",
      "layers.15.self_attn.v_proj.weight\n",
      "layers.15.self_attn.o_proj.weight\n",
      "layers.15.input_layernorm.weight\n",
      "layers.15.post_attention_layernorm.weight\n",
      "norm.weight\n",
      "lm_head.weight\n",
      "Llama model loaded from state dict\n",
      "LlamaModel(\n",
      "  (embed_tokens): Embedding(128256, 2048)\n",
      "  (layers): ModuleList(\n",
      "    (0-15): 16 x LlamaDecoderLayer(\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "        (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "        (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "      )\n",
      "      (self_attn): LlamaSdpaAttention(\n",
      "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "        (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm()\n",
      "      (post_attention_layernorm): LlamaRMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm()\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n",
      "1498482688\n"
     ]
    }
   ],
   "source": [
    "#Check that preload works\n",
    "\n",
    "model = llama3_aoc.LlamaModel.from_huggingface(configs.Llama1BConfig)\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "num_params = 0\n",
    "for p in model.parameters(): num_params += p.numel()\n",
    "print(num_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama model loaded from state dict\n",
      "AttentionOnlyLlamaModel(\n",
      "  (embed_tokens): Embedding(128256, 2048)\n",
      "  (layers): ModuleList(\n",
      "    (0-15): 16 x LlamaSdpaAttention(\n",
      "      (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "      (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "      (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm()\n",
      ")\n",
      "430442496\n"
     ]
    }
   ],
   "source": [
    "ao_model = llama3_aoc.AttentionOnlyLlamaModel.from_huggingface(configs.Llama1BConfig)\n",
    "\n",
    "print(ao_model)\n",
    "\n",
    "num_params = 0\n",
    "for p in ao_model.parameters(): num_params += p.numel()\n",
    "print(num_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test that Rotary projection works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 16])\n"
     ]
    }
   ],
   "source": [
    "def precompute_freqs_cis(dim: int, end: int, theta: float) -> torch.Tensor:\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device)  # type: ignore\n",
    "    freqs = torch.outer(t, freqs).float()  # type: ignore\n",
    "    return torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "\n",
    "freqs_cis = precompute_freqs_cis(dim=32, end=1024, theta=50000.0)\n",
    "print(freqs_cis.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 8, 32])\n",
      "torch.Size([1, 10, 8, 16])\n",
      "torch.Size([1, 10, 8, 32])\n",
      "tensor([[ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000],\n",
      "        [-0.3012,  1.3818,  0.3866,  1.3604,  0.7110,  1.2225,  0.8602,  1.1225,\n",
      "          0.9309,  1.0646,  0.9654,  1.0334,  0.9826,  1.0171,  0.9912,  1.0088,\n",
      "          0.9955,  1.0045,  0.9977,  1.0023,  0.9988,  1.0012,  0.9994,  1.0006,\n",
      "          0.9997,  1.0003,  0.9998,  1.0002,  0.9999,  1.0001,  1.0000,  1.0000],\n",
      "        [-1.3254,  0.4932, -0.3247,  1.3764,  0.3748,  1.3637,  0.7056,  1.2256,\n",
      "          0.8577,  1.1244,  0.9297,  1.0656,  0.9648,  1.0340,  0.9823,  1.0174,\n",
      "          0.9910,  1.0089,  0.9954,  1.0045,  0.9977,  1.0023,  0.9988,  1.0012,\n",
      "          0.9994,  1.0006,  0.9997,  1.0003,  0.9998,  1.0002,  0.9999,  1.0001],\n",
      "        [-1.1311, -0.8489, -0.9538,  1.0442,  0.0136,  1.4141,  0.5388,  1.3075,\n",
      "          0.7807,  1.1792,  0.8930,  1.0966,  0.9468,  1.0505,  0.9733,  1.0260,\n",
      "          0.9865,  1.0133,  0.9932,  1.0068,  0.9965,  1.0035,  0.9982,  1.0018,\n",
      "          0.9991,  1.0009,  0.9995,  1.0005,  0.9998,  1.0002,  0.9999,  1.0001],\n",
      "        [ 0.1032, -1.4104, -1.3415,  0.4477, -0.3485,  1.3706,  0.3627,  1.3669,\n",
      "          0.7001,  1.2288,  0.8552,  1.1264,  0.9285,  1.0667,  0.9642,  1.0346,\n",
      "          0.9820,  1.0177,  0.9909,  1.0091,  0.9954,  1.0046,  0.9976,  1.0023,\n",
      "          0.9988,  1.0012,  0.9994,  1.0006,  0.9997,  1.0003,  0.9998,  1.0002],\n",
      "        [ 1.2426, -0.6753, -1.3897, -0.2621, -0.6874,  1.2359,  0.1803,  1.4027,\n",
      "          0.6164,  1.2728,  0.8164,  1.1548,  0.9099,  1.0826,  0.9551,  1.0430,\n",
      "          0.9774,  1.0221,  0.9886,  1.0113,  0.9942,  1.0058,  0.9971,  1.0029,\n",
      "          0.9985,  1.0015,  0.9992,  1.0008,  0.9996,  1.0004,  0.9998,  1.0002],\n",
      "        [ 1.2396,  0.6808, -1.0862, -0.9056, -0.9806,  1.0190, -0.0051,  1.4142,\n",
      "          0.5300,  1.3111,  0.7766,  1.1819,  0.8910,  1.0982,  0.9459,  1.0513,\n",
      "          0.9728,  1.0265,  0.9863,  1.0136,  0.9930,  1.0069,  0.9965,  1.0035,\n",
      "          0.9982,  1.0018,  0.9991,  1.0009,  0.9995,  1.0005,  0.9998,  1.0002],\n",
      "        [ 0.0969,  1.4109, -0.5078, -1.3199, -1.2086,  0.7344, -0.1905,  1.4013,\n",
      "          0.4412,  1.3436,  0.7360,  1.2076,  0.8719,  1.1134,  0.9366,  1.0596,\n",
      "          0.9682,  1.0308,  0.9840,  1.0158,  0.9919,  1.0081,  0.9959,  1.0041,\n",
      "          0.9979,  1.0021,  0.9989,  1.0011,  0.9995,  1.0005,  0.9997,  1.0003],\n",
      "        [-1.1349,  0.8439,  0.1991, -1.4001, -1.3562,  0.4009, -0.3726,  1.3642,\n",
      "          0.3504,  1.3701,  0.6945,  1.2319,  0.8525,  1.1284,  0.9272,  1.0678,\n",
      "          0.9636,  1.0351,  0.9816,  1.0180,  0.9907,  1.0092,  0.9953,  1.0047,\n",
      "          0.9976,  1.0024,  0.9988,  1.0012,  0.9994,  1.0006,  0.9997,  1.0003],\n",
      "        [-1.3232, -0.4990,  0.8556, -1.1260, -1.4136,  0.0407, -0.5483,  1.3036,\n",
      "          0.2581,  1.3905,  0.6522,  1.2548,  0.8329,  1.1429,  0.9178,  1.0759,\n",
      "          0.9590,  1.0394,  0.9793,  1.0203,  0.9895,  1.0104,  0.9947,  1.0053,\n",
      "          0.9973,  1.0027,  0.9986,  1.0014,  0.9993,  1.0007,  0.9996,  1.0004]])\n"
     ]
    }
   ],
   "source": [
    "def apply_rotary_emb(\n",
    "    x: torch.Tensor,\n",
    "    freqs_cis: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    print(x.shape)\n",
    "\n",
    "    x_ = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "    print(x_.shape)\n",
    "\n",
    "    freqs_cis = freqs_cis[None,:,  None, :]\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis).flatten(-2)\n",
    "    print(x_out.shape)\n",
    "\n",
    "    return x_out.type_as(x)\n",
    "\n",
    "# bsz, n_head, toks, head_dim\n",
    "x = torch.ones([1,10,8,32])\n",
    "\n",
    "\n",
    "x = apply_rotary_emb(x, freqs_cis[:10,:])\n",
    "print(x[0,:,0,:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that text generation is the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128000, 8538, 10507, 315, 3062, 63119, 304, 4221, 61966, 527]\n",
      "<|begin_of_text|>Some examples of important benchmarks in language modelling are\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelpath)\n",
    "\n",
    "\n",
    "text = \"Some examples of important benchmarks in language modelling are\"\n",
    "\n",
    "tokens = tokenizer(text)\n",
    "tokens = tokens['input_ids']\n",
    "\n",
    "print(tokens)\n",
    "print(tokenizer.decode(tokens))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000,   8538,  10507,    315,   3062,  63119,    304,   4221,  61966,\n",
      "            527,   6814,  13850,     11,    220,    220,    335,     12,    457,\n",
      "             53,     17,     13,   4702,   1306,    279,    323,    285,    482,\n",
      "            842,     13,    578,    471,     11,    452,    574,    471,     11,\n",
      "            457,    335,     12,    586]])\n",
      "<|begin_of_text|>Some examples of important benchmarks in language modelling are pubulous,   }- }\n",
      "V2. Just after the andis - end. The return, N was return, }\n",
      " }- public\n"
     ]
    }
   ],
   "source": [
    "# Custom model generation\n",
    "toks = torch.tensor(tokens).unsqueeze(0).to(dtype=torch.long, device=device)\n",
    "logits = model(toks)\n",
    "\n",
    "output = model.generate(toks, 30, top_k=5).to('cpu')\n",
    "print(output)\n",
    "print(tokenizer.decode(output.flatten().tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some examples of important benchmarks in language modelling are:\n",
      "\n",
      "1. **BERT (Bidirectional Encoder Representations from Transformers)**: Developed by Google, BERT is a language model that achieves state-of-the-art results in many NLP tasks, including question\n"
     ]
    }
   ],
   "source": [
    "# HF model generation\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "generate_ids = hf_model.generate(\n",
    "    inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    max_length=50)\n",
    "\n",
    "print(tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
